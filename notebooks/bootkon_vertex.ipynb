{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad2b88b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 4: Machine Learning with Vertex AI\n",
    "\n",
    "Author: \n",
    "* Fabian Hirschmann <<fhirschmann@google.com>>\n",
    "\n",
    "Welcome back ðŸ‘‹ðŸ˜. During this lab, you will train a machine learning model on the data set you already know. We will deploy it to Vertex AI and finally construct a machine learning pipeline to perform the training process automatically.\n",
    "\n",
    "We will do it in three different maturity levels:\n",
    "\n",
    "1. Deploying locally trained models to Vertex AI using prebuilt containers\n",
    "2. Train and deploy model on Vertex AI using custom containers\n",
    "3. Use Vertex AI pipeline to train and deploy the model\n",
    "\n",
    "In this Jupyter Notebook, you can press `Shift + Return` to execute the current code junk and jump to the next one.\n",
    "\n",
    "## Step 1: Import Dependencies and Set Environment Variables\n",
    "\n",
    "Before we begin, let's import the necessary Python libraries and set a few environment variables for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf0314c-770c-41fe-be57-f41eb17bf48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from sklearn.metrics import roc_curve, auc as auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "import google.cloud.logging\n",
    "google.cloud.logging.Client().setup_logging(log_level=logging.WARNING)\n",
    "\n",
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "\n",
    "REGION = \"us-central1\"\n",
    "BQ_DATASET = \"ml_datasets\"\n",
    "BQ_TABLE = \"ulb_fraud_detection_dataproc\"\n",
    "BQ_SOURCE = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
    "PIPELINE_ROOT = f\"gs://{PROJECT_ID}-bucket/pipelines\"\n",
    "TRAIN_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bootkon/bootkon-train:latest\"\n",
    "PREDICT_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bootkon/bootkon-predict:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b46cf-c829-4439-9151-3722c290d5b8",
   "metadata": {},
   "source": [
    "If you get warnings about GPUs not being available -- that's fine.\n",
    "\n",
    "The requirements were automatically installed by the `bootstrap_workbench.sh` script we specified when we created this Workbench instance. However, if the above command fails due to import errors, uncomment the next chunk, run it, and then restart the kernel (`Kernel > Restart Kernel` in the menu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caad0bb3-7432-45b5-bf1a-eb7c0214b5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9ce43-7912-4fcc-98ee-e30ae961e43e",
   "metadata": {},
   "source": [
    "## Step 2: Create dataset for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8cb35-8c90-4378-a592-fd28ffa6d1a5",
   "metadata": {},
   "source": [
    "We initialize the AI Platform and BigQuery client to interact with Google Cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ea1163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"{PROJECT_ID}-bucket\")\n",
    "bq = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c15d1c-82ed-460c-80d3-c08c5bee1432",
   "metadata": {},
   "source": [
    "The BigQuery table we'll be working with is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e001be2f-5701-497d-b4a4-9eb8d2202fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astute-ace-336608.ml_datasets.ulb_fraud_detection_dataproc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BQ_SOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46092b6-2908-4bf6-b45f-53697f2b17c6",
   "metadata": {},
   "source": [
    "We execute a query to fetch the dataset from BigQuery and store it in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07455370-2100-469e-9a87-09963c593437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = bq.query(f\"SELECT * FROM `{BQ_SOURCE}`\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3aaa1-4083-4e35-8c70-6ecbf41fac76",
   "metadata": {},
   "source": [
    "Let's have a look at the data set in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590bc934-a254-4ee3-9dc4-1a21cf57648d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282.0</td>\n",
       "      <td>-0.356466</td>\n",
       "      <td>0.725418</td>\n",
       "      <td>1.971749</td>\n",
       "      <td>0.831343</td>\n",
       "      <td>0.369681</td>\n",
       "      <td>-0.107776</td>\n",
       "      <td>0.751610</td>\n",
       "      <td>-0.120166</td>\n",
       "      <td>-0.420675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020804</td>\n",
       "      <td>0.424312</td>\n",
       "      <td>-0.015989</td>\n",
       "      <td>0.466754</td>\n",
       "      <td>-0.809962</td>\n",
       "      <td>0.657334</td>\n",
       "      <td>-0.043150</td>\n",
       "      <td>-0.046401</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14332.0</td>\n",
       "      <td>1.071950</td>\n",
       "      <td>0.340678</td>\n",
       "      <td>1.784068</td>\n",
       "      <td>2.846396</td>\n",
       "      <td>-0.751538</td>\n",
       "      <td>0.403028</td>\n",
       "      <td>-0.734920</td>\n",
       "      <td>0.205807</td>\n",
       "      <td>1.092726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169632</td>\n",
       "      <td>-0.113604</td>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.468669</td>\n",
       "      <td>0.223541</td>\n",
       "      <td>-0.112355</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.021504</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32799.0</td>\n",
       "      <td>1.153477</td>\n",
       "      <td>-0.047859</td>\n",
       "      <td>1.358363</td>\n",
       "      <td>1.480620</td>\n",
       "      <td>-1.222598</td>\n",
       "      <td>-0.481690</td>\n",
       "      <td>-0.654461</td>\n",
       "      <td>0.128115</td>\n",
       "      <td>0.907095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125514</td>\n",
       "      <td>0.480049</td>\n",
       "      <td>-0.025964</td>\n",
       "      <td>0.701843</td>\n",
       "      <td>0.417245</td>\n",
       "      <td>-0.257691</td>\n",
       "      <td>0.060115</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35799.0</td>\n",
       "      <td>-0.769798</td>\n",
       "      <td>0.622325</td>\n",
       "      <td>0.242491</td>\n",
       "      <td>-0.586652</td>\n",
       "      <td>0.527819</td>\n",
       "      <td>-0.104512</td>\n",
       "      <td>0.209909</td>\n",
       "      <td>0.669861</td>\n",
       "      <td>-0.304509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152738</td>\n",
       "      <td>0.255654</td>\n",
       "      <td>-0.130237</td>\n",
       "      <td>-0.660934</td>\n",
       "      <td>-0.493374</td>\n",
       "      <td>0.331855</td>\n",
       "      <td>-0.011101</td>\n",
       "      <td>0.049089</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36419.0</td>\n",
       "      <td>1.047960</td>\n",
       "      <td>0.145048</td>\n",
       "      <td>1.624573</td>\n",
       "      <td>2.932652</td>\n",
       "      <td>-0.726574</td>\n",
       "      <td>0.690451</td>\n",
       "      <td>-0.627288</td>\n",
       "      <td>0.278709</td>\n",
       "      <td>0.318434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078499</td>\n",
       "      <td>0.658942</td>\n",
       "      <td>-0.067810</td>\n",
       "      <td>0.476882</td>\n",
       "      <td>0.526830</td>\n",
       "      <td>0.219902</td>\n",
       "      <td>0.070627</td>\n",
       "      <td>0.028488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>154599.0</td>\n",
       "      <td>0.667714</td>\n",
       "      <td>3.041502</td>\n",
       "      <td>-5.845112</td>\n",
       "      <td>5.967587</td>\n",
       "      <td>0.213863</td>\n",
       "      <td>-1.462923</td>\n",
       "      <td>-2.688761</td>\n",
       "      <td>0.677764</td>\n",
       "      <td>-3.447596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329760</td>\n",
       "      <td>-0.941383</td>\n",
       "      <td>-0.006075</td>\n",
       "      <td>-0.958925</td>\n",
       "      <td>0.239298</td>\n",
       "      <td>-0.067356</td>\n",
       "      <td>0.821048</td>\n",
       "      <td>0.426175</td>\n",
       "      <td>6.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>90676.0</td>\n",
       "      <td>-2.405580</td>\n",
       "      <td>3.738235</td>\n",
       "      <td>-2.317843</td>\n",
       "      <td>1.367442</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>1.919938</td>\n",
       "      <td>-3.106942</td>\n",
       "      <td>-10.764403</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>10.005998</td>\n",
       "      <td>-2.454964</td>\n",
       "      <td>1.684957</td>\n",
       "      <td>0.118263</td>\n",
       "      <td>-1.531380</td>\n",
       "      <td>-0.695308</td>\n",
       "      <td>-0.152502</td>\n",
       "      <td>-0.138866</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>34634.0</td>\n",
       "      <td>0.333499</td>\n",
       "      <td>1.699873</td>\n",
       "      <td>-2.596561</td>\n",
       "      <td>3.643945</td>\n",
       "      <td>-0.585068</td>\n",
       "      <td>-0.654659</td>\n",
       "      <td>-2.275789</td>\n",
       "      <td>0.675229</td>\n",
       "      <td>-2.042416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469212</td>\n",
       "      <td>-0.144363</td>\n",
       "      <td>-0.317981</td>\n",
       "      <td>-0.769644</td>\n",
       "      <td>0.807855</td>\n",
       "      <td>0.228164</td>\n",
       "      <td>0.551002</td>\n",
       "      <td>0.305473</td>\n",
       "      <td>18.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>96135.0</td>\n",
       "      <td>-1.952933</td>\n",
       "      <td>3.541385</td>\n",
       "      <td>-1.310561</td>\n",
       "      <td>5.955664</td>\n",
       "      <td>-1.003993</td>\n",
       "      <td>0.983049</td>\n",
       "      <td>-4.587235</td>\n",
       "      <td>-4.892184</td>\n",
       "      <td>-2.516752</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.998091</td>\n",
       "      <td>1.133706</td>\n",
       "      <td>-0.041461</td>\n",
       "      <td>-0.215379</td>\n",
       "      <td>-0.865599</td>\n",
       "      <td>0.212545</td>\n",
       "      <td>0.532897</td>\n",
       "      <td>0.357892</td>\n",
       "      <td>18.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0          282.0 -0.356466  0.725418  1.971749  0.831343  0.369681 -0.107776   \n",
       "1        14332.0  1.071950  0.340678  1.784068  2.846396 -0.751538  0.403028   \n",
       "2        32799.0  1.153477 -0.047859  1.358363  1.480620 -1.222598 -0.481690   \n",
       "3        35799.0 -0.769798  0.622325  0.242491 -0.586652  0.527819 -0.104512   \n",
       "4        36419.0  1.047960  0.145048  1.624573  2.932652 -0.726574  0.690451   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  154599.0  0.667714  3.041502 -5.845112  5.967587  0.213863 -1.462923   \n",
       "284803   90676.0 -2.405580  3.738235 -2.317843  1.367442  0.394001  1.919938   \n",
       "284804   34634.0  0.333499  1.699873 -2.596561  3.643945 -0.585068 -0.654659   \n",
       "284805   96135.0 -1.952933  3.541385 -1.310561  5.955664 -1.003993  0.983049   \n",
       "284806    4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "\n",
       "              V7         V8        V9  ...        V21       V22       V23  \\\n",
       "0       0.751610  -0.120166 -0.420675  ...   0.020804  0.424312 -0.015989   \n",
       "1      -0.734920   0.205807  1.092726  ...  -0.169632 -0.113604  0.067643   \n",
       "2      -0.654461   0.128115  0.907095  ...   0.125514  0.480049 -0.025964   \n",
       "3       0.209909   0.669861 -0.304509  ...   0.152738  0.255654 -0.130237   \n",
       "4      -0.627288   0.278709  0.318434  ...   0.078499  0.658942 -0.067810   \n",
       "...          ...        ...       ...  ...        ...       ...       ...   \n",
       "284802 -2.688761   0.677764 -3.447596  ...   0.329760 -0.941383 -0.006075   \n",
       "284803 -3.106942 -10.764403  3.353525  ...  10.005998 -2.454964  1.684957   \n",
       "284804 -2.275789   0.675229 -2.042416  ...   0.469212 -0.144363 -0.317981   \n",
       "284805 -4.587235  -4.892184 -2.516752  ...  -1.998091  1.133706 -0.041461   \n",
       "284806  0.562320  -0.399147 -0.238253  ...  -0.294166 -0.932391  0.172726   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.466754 -0.809962  0.657334 -0.043150 -0.046401    0.00      0  \n",
       "1       0.468669  0.223541 -0.112355  0.014015  0.021504    0.00      0  \n",
       "2       0.701843  0.417245 -0.257691  0.060115  0.035332    0.00      0  \n",
       "3      -0.660934 -0.493374  0.331855 -0.011101  0.049089    0.00      0  \n",
       "4       0.476882  0.526830  0.219902  0.070627  0.028488    0.00      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "284802 -0.958925  0.239298 -0.067356  0.821048  0.426175    6.74      1  \n",
       "284803  0.118263 -1.531380 -0.695308 -0.152502 -0.138866    6.99      1  \n",
       "284804 -0.769644  0.807855  0.228164  0.551002  0.305473   18.96      1  \n",
       "284805 -0.215379 -0.865599  0.212545  0.532897  0.357892   18.96      1  \n",
       "284806 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02afc5-90bb-479d-bd8e-40f0d0eb4c9b",
   "metadata": {},
   "source": [
    "We separate the target variable (`Class`), which we want to predict, from the features (all other columns). The `Class` column indicates whether a transaction is fraudulent (1) or legitimate (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5707b149-4f88-4375-ba9b-878781b57eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = data[\"Class\"].astype(int)\n",
    "data.drop(\"Class\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c56564-6a0a-4726-8d55-ed7f3730dfa2",
   "metadata": {},
   "source": [
    "Fraud detection datasets are typically highly imbalanced, meaning the majority of transactions are legitimate. We check the distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfefa617-6cd7-40f3-81ae-4aa1c2aa2ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32d7aa-bf49-4767-a588-22ab47d78a14",
   "metadata": {},
   "source": [
    "We split our dataset into two parts:\n",
    "\n",
    "- Training set (80%): Used to train the machine learning model.\n",
    "- Testing set (20%): Used to evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec82ed34-ae5d-4102-a67d-b3f6956a435c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00319219-04c0-4f9c-a59e-eaa3809ef84b",
   "metadata": {},
   "source": [
    "Let's also save it to Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b804dd-9d24-46d4-8f23-cf2b1340dbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\", index=False)\n",
    "y_train.to_frame().to_csv(f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7627f-aacd-401f-b32b-d39f690c3172",
   "metadata": {},
   "source": [
    "## Step 3.1: Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9bdfa-a156-45db-b466-2a10e1539a78",
   "metadata": {},
   "source": [
    "We use a `RandomForestClassifier`, which is an ensemble learning method that creates multiple decision trees and aggregates their predictions. This helps improve accuracy and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f91fb92-47a9-4e93-8a68-35974c0fbca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   38.7s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:   56.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=8, verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7cd00-ded8-4eac-a4a4-6ed12abc0ce0",
   "metadata": {},
   "source": [
    "We calculate the accuracy of the model, which measures the proportion of correctly classified instances.\n",
    "\n",
    "For a highly imbalanced data set, the accuracy is often meaningless, because a simple classifier that always says ***not fraud*** will have an accuracy close to 1 already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35eb9ac3-0897-429d-b2cc-4e4d440cb648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995435553526912"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e458e-03a3-4d2a-9b97-2b4760ac7118",
   "metadata": {},
   "source": [
    "We compute the ROC AUC (Receiver Operating Characteristic - Area Under the Curve) score. This metric evaluates the model's ability to distinguish between classes. A score closer to 1 indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb3e080-b7ff-46c6-aff0-7da10ed4b76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416200075786283"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb1b11-aba7-46a4-89a7-0c020d34fef9",
   "metadata": {},
   "source": [
    "We save the trained model to a local file so we can deploy it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b1c581a-7f80-4582-9c56-bf3a3563001a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5826c-7da4-42f8-bcc6-dd5188acbf59",
   "metadata": {},
   "source": [
    "We upload the trained model to Vertex AI, where it can be used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a9fe3-1f24-4fef-a870-d49f4fd99d37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.3 MiB/  1.3 MiB]                                                \n",
      "Operation completed over 1 objects/1.3 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.joblib gs://{PROJECT_ID}-bucket/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150706a1-c207-4051-94e2-b8acac8387e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3.2: Serve locally trained model on Vertex AI\n",
    "\n",
    "The Vertex AI Model Registry is a centralized repository in Google Cloud's Vertex AI platform where machine learning (ML) models are stored, managed, and versioned. It allows data scientists and ML engineers to track different model versions, store metadata, and deploy models seamlessly to Vertex AI endpoints for inference.\n",
    "\n",
    "Key features of the Model Registry include:\n",
    "\n",
    "* Model Versioning: Track multiple versions of a model.\n",
    "* Metadata Management: Store details such as model parameters, training data, and performance metrics.\n",
    "* Deployment & Serving: Deploy registered models to Vertex AI Endpoints, Batch Predictions, or export them for external use.\n",
    "* Model Governance: Manage access control, approval workflows, and lineage tracking.\n",
    "* Integration with Pipelines: Automate model registration via Vertex AI Pipelines.\n",
    "\n",
    "We can register the model we just trained in this notebook as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf07af9-6490-4a6e-9ff8-6304eeb5feff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/888342260584/locations/us-central1/models/318714336031801344/operations/3251130435928850432\n",
      "Model created. Resource name: projects/888342260584/locations/us-central1/models/318714336031801344@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/888342260584/locations/us-central1/models/318714336031801344@1')\n"
     ]
    }
   ],
   "source": [
    "vertex_model_upload = aiplatform.Model.upload(\n",
    "    display_name=\"bootkon-upload-model\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest\",\n",
    "    artifact_uri=f\"gs://{PROJECT_ID}-bucket/model/\",\n",
    "    is_default_version=True,\n",
    "    version_aliases=[\"v1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc86b2-ddfb-4b7b-a129-e3127a6c7362",
   "metadata": {},
   "source": [
    "Once the model has been uploaded, navigate to the [`Model Registry` in Vertex AI](https://console.cloud.google.com/vertex-ai/models). Click on `bootkon-model`. Can you find your newly created model artifact? Open the `VERSION DETAILS` tab and try to find your model artifact on Cloud Storage.\n",
    "\n",
    "Let's deploy the model to an endpoint for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6e9ab8-e08b-453f-85be-59b95c933b73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/888342260584/locations/us-central1/endpoints/3819255893661319168/operations/7862816454356238336\n",
      "Endpoint created. Resource name: projects/888342260584/locations/us-central1/endpoints/3819255893661319168\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/888342260584/locations/us-central1/endpoints/3819255893661319168')\n"
     ]
    }
   ],
   "source": [
    "endpoint_upload = aiplatform.Endpoint.create(display_name=\"bootkon-endpoint-upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d151a-c23e-4da0-923c-7bb3cc807972",
   "metadata": {},
   "source": [
    "The next code chunk will take around 10min. We don't want to wait for that, so we set `sync=False` and look at the result later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40f9482d-125c-43fd-a5fb-2042cd6f4b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/888342260584/locations/us-central1/endpoints/3819255893661319168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f5612fbbee0> \n",
       "resource name: projects/888342260584/locations/us-central1/endpoints/3819255893661319168"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy Endpoint model backing LRO: projects/888342260584/locations/us-central1/endpoints/3819255893661319168/operations/3229456862722129920\n",
      "Endpoint model deployed. Resource name: projects/888342260584/locations/us-central1/endpoints/3819255893661319168\n",
      "Deploying model to Endpoint : projects/888342260584/locations/us-central1/endpoints/6327760886106685440\n",
      "Deploy Endpoint model backing LRO: projects/888342260584/locations/us-central1/endpoints/6327760886106685440/operations/8547363597716553728\n",
      "Endpoint model deployed. Resource name: projects/888342260584/locations/us-central1/endpoints/6327760886106685440\n"
     ]
    }
   ],
   "source": [
    "vertex_model_upload.deploy(\n",
    "    deployed_model_display_name=\"bootkon-model-upload\",\n",
    "    endpoint=endpoint_upload,\n",
    "    machine_type=\"n2-standard-2\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581c19-50bd-4fc2-be82-9bc564c4baca",
   "metadata": {},
   "source": [
    "The next chunk lists the currently deployed models. While the model is deploying, it wont's show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d9d2a32-7847-47a1-aa44-ed9f41f538b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_upload.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2828f-8fc4-4f95-bd02-04995afd9aa8",
   "metadata": {},
   "source": [
    "## Step 4: Train and serve model using custom containers\n",
    "\n",
    "In this section, we will train a `RandomForestClassifier` using **custom containers** on Vertex AI and deploy it for real-time predictions. Instead of using pre-built containers, we will package our training and prediction logic into Docker containers, allowing for **full control over dependencies, runtime environments, and scalability**. \n",
    "\n",
    "The process consists of two main steps:\n",
    "1. **Model Training:** We will preprocess the dataset, train a model and save it as a serialized `joblib` file. The trained model will be uploaded to Cloud Storage for deployment.\n",
    "2. **Model Serving:** Using a separate container, the stored model will be loaded from Cloud Storage, and an API will be exposed via Flask (or **FastAPI** in production) to handle inference requests.\n",
    "\n",
    "By leveraging Vertex AIâ€™s custom training and prediction services, we can achieve a **scalable, managed ML workflow** while keeping complete flexibility over the training and deployment pipeline.\n",
    "\n",
    "We will create the following files:\n",
    "\n",
    "- `train/Dockerfile`: Dockerfile for the training container\n",
    "- `train/train.py`: Training script\n",
    "- `predict/Dockerfile`: Dockerfile for the prediction container\n",
    "- `predict/predict.py`: Prediction script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa8927f0-3c70-4d10-a353-4f223ffb5f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738836631.585455 2291521 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "mkdir -p train predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bb918e4-b734-4975-9f6a-bde944794b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY train.py /app/train.py\n",
    "\n",
    "RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage fsspec gcsfs\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/app/train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa30621-9df7-43ab-abcd-64f6c5b6f1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict/Dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY predict.py /app/predict.py\n",
    "\n",
    "RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage google-cloud-aiplatform fsspec gcsfs flask\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"/app/predict.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aabe48-5c59-48f7-afed-0b7c4c40eea7",
   "metadata": {},
   "source": [
    "The `train.py` script trains a `RandomForestClassifier` using scikit-learn, saves it as a `joblib` file, and uploads it to Cloud Storage. It reads the training data (`X_train` and `y_train`) from CSV files provided as command-line arguments and retrieves the target storage directory from the `AIP_MODEL_DIR` environment variable. The trained model is stored in GCS for later deployment on Vertex AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af82d431-2684-4364-a0fe-01610e9ea191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/train.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from google.cloud import storage\n",
    "\n",
    "AIP_MODEL_DIR = os.environ[\"AIP_MODEL_DIR\"]\n",
    "\n",
    "X_train = pd.read_csv(sys.argv[1])\n",
    "y_train = pd.read_csv(sys.argv[2])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=8, verbose=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, \"model.joblib\")\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(AIP_MODEL_DIR.split(\"/\")[2])\n",
    "blob = bucket.blob(\"/\".join(AIP_MODEL_DIR.split(\"/\")[3:]) + \"/model.joblib\")\n",
    "blob.upload_from_filename(\"model.joblib\")\n",
    "print(f\"Wrote model to {AIP_MODEL_DIR}/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5d45d-be53-405c-994b-b9367f6793a9",
   "metadata": {},
   "source": [
    "The `predict.py` script is a flask-based prediction server designed for deployment on Vertex AI using custom containers. It retrieves the model artifacts from Cloud Storage using `prediction_utils.download_model_artifacts()`, loads the model with `joblib`, and exposes two API endpoints:\n",
    "\n",
    "- **`/predict`** for inference  \n",
    "- **`/health`** for monitoring the service status  \n",
    "\n",
    "The script reads environment variables such as `AIP_STORAGE_URI` for downloading the model and `AIP_PREDICT_ROUTE` for defining the prediction route dynamically. \n",
    "\n",
    "âš  **In production,** it is recommended to use **FastAPI** instead of Flask due to its superior performance, asynchronous capabilities, and built-in request validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63cccaed-feed-4e7d-a653-5b2ec5501aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict/predict.py\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import flask\n",
    "import numpy as np\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "AIP_STORAGE_URI = os.environ[\"AIP_STORAGE_URI\"]\n",
    "print(f\"Downloading model from {AIP_STORAGE_URI}/model.joblib\")\n",
    "prediction_utils.download_model_artifacts(AIP_STORAGE_URI)\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "@app.route(os.environ.get(\"AIP_PREDICT_ROUTE\", \"/predict\"), methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = flask.request.get_json()\n",
    "    inputs = np.array(data[\"instances\"])\n",
    "    predictions = model.predict(inputs).tolist()\n",
    "    return flask.jsonify({\"predictions\": predictions})\n",
    "\n",
    "@app.route(os.environ.get(\"AIP_HEALTH_ROUTE\", \"/health\"), methods=[\"GET\"])\n",
    "def health_check():\n",
    "    print(\"Received health check\")\n",
    "    return flask.jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=int(os.environ.get(\"AIP_HTTP_PORT\", 8080)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba306444-44bb-464e-b554-c043e45ea557",
   "metadata": {},
   "source": [
    "We will store the container images in a docker repository named `bootkon`. Let's create it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55c6a141-8473-4483-809f-56ea486a02d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create bootkon --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39408f-a804-4b0d-9ec7-9d3399d9fce9",
   "metadata": {},
   "source": [
    "We can use Cloud Build to build to image and automatically push it to the container repository we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b3cefa6-5d11-46be-9e6a-9d4209afb46b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 881 bytes before compression.\n",
      "Uploading tarball of [.] to [gs://astute-ace-336608_cloudbuild/source/1738836633.861682-dfcade422db44a01a5cdc99b19e328ae.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/astute-ace-336608/locations/us-central1/builds/656a6cd8-c512-4525-8015-041aa45910a1].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/656a6cd8-c512-4525-8015-041aa45910a1?project=888342260584 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"656a6cd8-c512-4525-8015-041aa45910a1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://astute-ace-336608_cloudbuild/source/1738836633.861682-dfcade422db44a01a5cdc99b19e328ae.tgz#1738836634166283\n",
      "Copying gs://astute-ace-336608_cloudbuild/source/1738836633.861682-dfcade422db44a01a5cdc99b19e328ae.tgz#1738836634166283...\n",
      "/ [1 files][  719.0 B/  719.0 B]                                                \n",
      "Operation completed over 1 objects/719.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/5 : FROM python:3.10-slim\n",
      "3.10-slim: Pulling from library/python\n",
      "c29f5b76f736: Pulling fs layer\n",
      "74e68b11a1c1: Pulling fs layer\n",
      "a477a912afa7: Pulling fs layer\n",
      "8c67a072a8ad: Pulling fs layer\n",
      "8c67a072a8ad: Waiting\n",
      "74e68b11a1c1: Verifying Checksum\n",
      "74e68b11a1c1: Download complete\n",
      "a477a912afa7: Verifying Checksum\n",
      "a477a912afa7: Download complete\n",
      "8c67a072a8ad: Verifying Checksum\n",
      "8c67a072a8ad: Download complete\n",
      "c29f5b76f736: Verifying Checksum\n",
      "c29f5b76f736: Download complete\n",
      "c29f5b76f736: Pull complete\n",
      "74e68b11a1c1: Pull complete\n",
      "a477a912afa7: Pull complete\n",
      "8c67a072a8ad: Pull complete\n",
      "Digest: sha256:66aad90b231f011cb80e1966e03526a7175f0586724981969b23903abac19081\n",
      "Status: Downloaded newer image for python:3.10-slim\n",
      " ---> b791f5ccaef8\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Running in 56016db77b55\n",
      "Removing intermediate container 56016db77b55\n",
      " ---> d2aee7374fd5\n",
      "Step 3/5 : COPY train.py /app/train.py\n",
      " ---> cf416cdcb812\n",
      "Step 4/5 : RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage fsspec gcsfs\n",
      " ---> Running in 71fdf9258990\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 71fdf9258990\n",
      " ---> 011c41f54d22\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"/app/train.py\"]\n",
      " ---> Running in 0220c09668a3\n",
      "Removing intermediate container 0220c09668a3\n",
      " ---> 9ba20584878f\n",
      "Successfully built 9ba20584878f\n",
      "Successfully tagged us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train]\n",
      "d64bc8525fd2: Preparing\n",
      "2a05a6dc2b4d: Preparing\n",
      "939a29671340: Preparing\n",
      "cd9604f50b89: Preparing\n",
      "f52fdf687483: Preparing\n",
      "93d4d0473476: Preparing\n",
      "7914c8f600f5: Preparing\n",
      "93d4d0473476: Waiting\n",
      "7914c8f600f5: Waiting\n",
      "f52fdf687483: Layer already exists\n",
      "cd9604f50b89: Layer already exists\n",
      "93d4d0473476: Layer already exists\n",
      "2a05a6dc2b4d: Pushed\n",
      "939a29671340: Pushed\n",
      "7914c8f600f5: Pushed\n",
      "d64bc8525fd2: Pushed\n",
      "latest: digest: sha256:85766b0e9585e799641acb60805472753c0e0ff3316a4358a8eb847d8b57918c size: 1786\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                                        STATUS\n",
      "656a6cd8-c512-4525-8015-041aa45910a1  2025-02-06T10:10:34+00:00  1M28S     gs://astute-ace-336608_cloudbuild/source/1738836633.861682-dfcade422db44a01a5cdc99b19e328ae.tgz  us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-train (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd train && gcloud builds submit --region={REGION} --tag={TRAIN_IMAGE_URI} --timeout=1h --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff5982-15a5-4dcb-9685-c669a22381dc",
   "metadata": {},
   "source": [
    "Let's do the same thing for the prediction image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "515e0f7c-43ff-454f-83b8-7c353b374a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 3 file(s) totalling 1.2 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://astute-ace-336608_cloudbuild/source/1738836726.544182-db4ad4126e51460c93797669623f1005.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/astute-ace-336608/locations/us-central1/builds/91513b24-21b6-45b5-b19d-7152521e5576].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/91513b24-21b6-45b5-b19d-7152521e5576?project=888342260584 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"91513b24-21b6-45b5-b19d-7152521e5576\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://astute-ace-336608_cloudbuild/source/1738836726.544182-db4ad4126e51460c93797669623f1005.tgz#1738836726853601\n",
      "Copying gs://astute-ace-336608_cloudbuild/source/1738836726.544182-db4ad4126e51460c93797669623f1005.tgz#1738836726853601...\n",
      "/ [1 files][  960.0 B/  960.0 B]                                                \n",
      "Operation completed over 1 objects/960.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  4.608kB\n",
      "Step 1/6 : FROM python:3.10-slim\n",
      "3.10-slim: Pulling from library/python\n",
      "c29f5b76f736: Pulling fs layer\n",
      "74e68b11a1c1: Pulling fs layer\n",
      "a477a912afa7: Pulling fs layer\n",
      "8c67a072a8ad: Pulling fs layer\n",
      "8c67a072a8ad: Waiting\n",
      "74e68b11a1c1: Verifying Checksum\n",
      "74e68b11a1c1: Download complete\n",
      "8c67a072a8ad: Verifying Checksum\n",
      "8c67a072a8ad: Download complete\n",
      "c29f5b76f736: Verifying Checksum\n",
      "c29f5b76f736: Download complete\n",
      "a477a912afa7: Verifying Checksum\n",
      "a477a912afa7: Download complete\n",
      "c29f5b76f736: Pull complete\n",
      "74e68b11a1c1: Pull complete\n",
      "a477a912afa7: Pull complete\n",
      "8c67a072a8ad: Pull complete\n",
      "Digest: sha256:66aad90b231f011cb80e1966e03526a7175f0586724981969b23903abac19081\n",
      "Status: Downloaded newer image for python:3.10-slim\n",
      " ---> b791f5ccaef8\n",
      "Step 2/6 : WORKDIR /app\n",
      " ---> Running in 162eda615b38\n",
      "Removing intermediate container 162eda615b38\n",
      " ---> 97658c6c8595\n",
      "Step 3/6 : COPY predict.py /app/predict.py\n",
      " ---> 470ef3a4b34f\n",
      "Step 4/6 : RUN pip install --no-cache-dir --quiet pandas scikit-learn==1.5.2 google-cloud-storage google-cloud-aiplatform fsspec gcsfs flask\n",
      " ---> Running in 045717734e45\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 045717734e45\n",
      " ---> edf409dd3006\n",
      "Step 5/6 : EXPOSE 8080\n",
      " ---> Running in dd396e296c1a\n",
      "Removing intermediate container dd396e296c1a\n",
      " ---> 80679c894b37\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"/app/predict.py\"]\n",
      " ---> Running in 0907cc782ea0\n",
      "Removing intermediate container 0907cc782ea0\n",
      " ---> 31a4fb8d09db\n",
      "Successfully built 31a4fb8d09db\n",
      "Successfully tagged us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict]\n",
      "502a68914f32: Preparing\n",
      "db0a40aaabaa: Preparing\n",
      "a27c6beaba72: Preparing\n",
      "cd9604f50b89: Preparing\n",
      "f52fdf687483: Preparing\n",
      "93d4d0473476: Preparing\n",
      "7914c8f600f5: Preparing\n",
      "93d4d0473476: Waiting\n",
      "7914c8f600f5: Waiting\n",
      "f52fdf687483: Layer already exists\n",
      "cd9604f50b89: Layer already exists\n",
      "93d4d0473476: Layer already exists\n",
      "a27c6beaba72: Pushed\n",
      "db0a40aaabaa: Pushed\n",
      "7914c8f600f5: Pushed\n",
      "502a68914f32: Pushed\n",
      "latest: digest: sha256:c3af4acd03da6c9aeb662d37c169ec786887689d70a2b0ac57aa2f69ce52994e size: 1786\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                                          STATUS\n",
      "91513b24-21b6-45b5-b19d-7152521e5576  2025-02-06T10:12:07+00:00  1M41S     gs://astute-ace-336608_cloudbuild/source/1738836726.544182-db4ad4126e51460c93797669623f1005.tgz  us-central1-docker.pkg.dev/astute-ace-336608/bootkon/bootkon-predict (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cd predict && gcloud builds submit --region={REGION} --tag={PREDICT_IMAGE_URI} --timeout=1h --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5ae53-f124-419a-b4a9-f6ca41b0fac7",
   "metadata": {},
   "source": [
    "Now that the container is ready, we can run it as `CustomContainerTrainingJob` -- giving the training data set as arguments. This will take around 5-10min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15a6594f-c9ed-40cc-a919-c7a92114ee4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name = \"bootkon-custom\",\n",
    "    container_uri = TRAIN_IMAGE_URI,\n",
    "    model_serving_container_image_uri = PREDICT_IMAGE_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c03b9420-e8ec-4b13-9f9e-9ae24a71a81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://astute-ace-336608-bucket/aiplatform-custom-training-2025-02-06-10:13:51.367 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2493735768145526784?project=888342260584\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5969929740289572864?project=888342260584\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/888342260584/locations/us-central1/trainingPipelines/2493735768145526784\n",
      "Model available at projects/888342260584/locations/us-central1/models/5059878843746091008\n"
     ]
    }
   ],
   "source": [
    "vertex_model_custom = job.run(\n",
    "    args=[\n",
    "        f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\",\n",
    "        f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7315374-cd73-4bd8-8e2c-20815afa917a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/888342260584/locations/us-central1/endpoints/6327760886106685440/operations/8250126022310100992\n",
      "Endpoint created. Resource name: projects/888342260584/locations/us-central1/endpoints/6327760886106685440\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/888342260584/locations/us-central1/endpoints/6327760886106685440')\n"
     ]
    }
   ],
   "source": [
    "endpoint_custom = aiplatform.Endpoint.create(display_name=\"bootkon-endpoint-custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a58fd6-c01f-45e7-b6ab-13a800f72896",
   "metadata": {},
   "source": [
    "We also deploy this model and don't wait for it to finish (`sync=False`) -- instead we come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02219939-8414-4bde-94ad-d3b97b003077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f5612fc3610> \n",
       "resource name: projects/888342260584/locations/us-central1/endpoints/6327760886106685440"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_model_custom.deploy(\n",
    "    deployed_model_display_name=\"bootkon-model-custom\",\n",
    "    endpoint=endpoint_custom,\n",
    "    machine_type=\"n2-standard-2\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770835f3-1772-472f-8cf3-9a777290e3dc",
   "metadata": {},
   "source": [
    "## Step 5: Train and deploy models using Vertex Pipelines\n",
    "\n",
    "In this section, we will train a `RandomForestClassifier` using Vertex AI Pipelines and Kubeflow Pipelines (KFP) and deploy it for real-time predictions. Unlike the custom container approach, we will define a pipeline-based workflow for automated training, model storage, and deployment. This approach allows us to achieve repeatability, scalability, and automation for end-to-end ML workflows on Google Cloud.\n",
    "\n",
    "By leveraging Vertex AI Pipelines, we can create a fully managed, automated ML pipeline that integrates seamlessly with GCP services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc9e9c47-a872-4523-988a-4a36185daae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl, compiler\n",
    "\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.dsl import importer_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb54bb-3b3c-4d2b-9ab5-cc2de258b502",
   "metadata": {},
   "source": [
    "Next, we create a Kubeflow pipeline that automates the training, model upload, and deployment process in Vertex AI.\n",
    "\n",
    "**Pipeline Steps**\n",
    "\n",
    "1. Define a Unique Model Directory:\n",
    "* The pipeline assigns a unique Cloud Storage path for storing the trained model using `PIPELINE_JOB_ID_PLACEHOLDER`, ensuring each run has an isolated model directory.\n",
    "\n",
    "2. Run a Custom Training Job\n",
    "* Uses `CustomTrainingJobOp` to launch a training job on Vertex AI.\n",
    "* The training script is executed inside a custom container (`TRAIN_IMAGE_URI`).\n",
    "* The trained model is stored in the dynamically created directory (`AIP_MODEL_DIR`).\n",
    "\n",
    "3. Import the Trained Model as an Artifact\n",
    "* The `importer_node.importer` step converts the saved model directory into an `UnmanagedContainerModel`, allowing it to be used by Vertex AI.\n",
    "\n",
    "4. Upload the Model to Vertex AI\n",
    "* The `ModelUploadOp` registers the trained model in Vertex AI, making it available for deployment.\n",
    "\n",
    "5. Create an Endpoint for Deployment\n",
    "* `EndpointCreateOp` initializes a new prediction endpoint in Vertex AI.\n",
    "\n",
    "6. Deploy the Model to the Endpoint\n",
    "* `ModelDeployOp` deploys the registered model to the created endpoint with a dedicated `n1-standard-4` machine.\n",
    "\n",
    "**Key Features**\n",
    "- **Dynamically generated model path** ensures each pipeline run has an isolated model storage.\n",
    "- **Custom container training** allows full control over the training process.\n",
    "- **Automated model registration and deployment** simplifies the end-to-end MLOps workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2797c69-cc93-40c1-89b9-af39003780f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"bootkon-pipeline\")\n",
    "def pipeline(\n",
    "    X_train: str,\n",
    "    y_train: str,\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = \"bootkon-pipeline\",\n",
    "):\n",
    "    model_dir = f\"{PIPELINE_ROOT}/model-{kfp.dsl.PIPELINE_JOB_ID_PLACEHOLDER}\"\n",
    "    custom_job_task = CustomTrainingJobOp(\n",
    "        project=project,\n",
    "        display_name=\"bootkon-model-pipeline\",\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"containerSpec\": {\n",
    "                    \"args\": [X_train, y_train],\n",
    "                    \"env\": [{\"name\": \"AIP_MODEL_DIR\", \"value\": model_dir}],\n",
    "                    \"imageUri\": TRAIN_IMAGE_URI,\n",
    "                },\n",
    "                \"replicaCount\": \"1\",\n",
    "                \"machineSpec\": {\n",
    "                    \"machineType\": \"n1-standard-4\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=model_dir,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": PREDICT_IMAGE_URI,\n",
    "            },\n",
    "        },\n",
    "    ).after(custom_job_task)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"bootkon-endpoint-pipeline\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b15037-7884-4993-91f0-a784cb93e60c",
   "metadata": {},
   "source": [
    "The following command compiles the `bootkon-pipeline` into a JSON file that can be submitted to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2db3cd1-8206-41cd-856a-3754b4d4c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"bootkon_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ef5ff-6ee9-41c0-bca7-b10b67a39ab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "And we submit it. Feel free to investigate the pipeline using the link that is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30b0b5e4-bf20-45d1-a9d1-35c39e1f563c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bootkon-pipeline-20250206102105?project=888342260584\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/888342260584/locations/us-central1/pipelineJobs/bootkon-pipeline-20250206102105\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"bootkon-pipeline\",\n",
    "    template_path=\"bootkon_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    project=PROJECT_ID,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"X_train\": f\"gs://{PROJECT_ID}-bucket/data/vertex/X_train.csv\",\n",
    "        \"y_train\": f\"gs://{PROJECT_ID}-bucket/data/vertex/y_train.csv\"\n",
    "    },\n",
    ")\n",
    "\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb4972-ec2f-4673-b19b-28915a1a79db",
   "metadata": {},
   "source": [
    "## Step 6: Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a56ffe-13f5-499c-a5ac-9e888f9d9239",
   "metadata": {},
   "source": [
    "We now should have several endpoints deployed. Let's check the endpoint from Step 3.2 (the ***upload*** model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "264ffe18-281c-4c2b-92a4-e74d447a0918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dedicated_resources {\n",
       "   machine_spec {\n",
       "     machine_type: \"n2-standard-2\"\n",
       "   }\n",
       "   min_replica_count: 1\n",
       "   max_replica_count: 1\n",
       " }\n",
       " id: \"4686361549651050496\"\n",
       " model: \"projects/888342260584/locations/us-central1/models/318714336031801344\"\n",
       " model_version_id: \"1\"\n",
       " display_name: \"bootkon-model-upload\"\n",
       " create_time {\n",
       "   seconds: 1738836631\n",
       "   nanos: 656011000\n",
       " }]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_upload.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cec05-ee1b-4566-89c0-5444ccaab4df",
   "metadata": {},
   "source": [
    "Let's make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3b95b55-5865-4049-bb5a-f21ea1b6d3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint_upload.predict(instances=X_test.head(4000).values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5feec-6866-4f96-b98e-d9d3b498f80d",
   "metadata": {},
   "source": [
    "Most of them are ***not fraud*** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dd3fc97-fc43-4eba-a819-337d23f3d942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145ffaa-8e0f-459c-93e3-dd637a308868",
   "metadata": {},
   "source": [
    "But there are also a few fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ef453a4-2254-4f0d-8fde-0568483ff063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320aaf99-c110-4e05-ae1a-f23a8b214b61",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can do the same thing with the other endpoint we created through custom containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d08f56b-8e1b-43e0-85fb-65c03c7eb40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = endpoint_custom.predict(instances=X_test.head(4000).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "737ca3bd-5e91-4b66-a6c2-c5007f951a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77729c4e-516d-4cc4-9226-b90660df3e3c",
   "metadata": {},
   "source": [
    "## Investigate results in the Cloud Console\n",
    "\n",
    "<font color=\"red\"><b>Great job deploying all these models. Now, please go back to the lab in Cloud Shell and continue from there!</b></font>\n",
    "\n",
    "<img src=\"../docs/img/lab4/cloud_shell_4.png\" width=300/>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
